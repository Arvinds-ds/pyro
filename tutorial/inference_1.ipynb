{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# import some dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference in Pyro: from stochastic functions to marginal distributions\n",
    "\n",
    "For non-primitive stochastic functions, we can no longer explicitly compute the probability of an output `y'` or draw samples from the marginal distribution over return values `p(y | x)`.\n",
    "\n",
    "*Inference* in Pyro is the problem of reifying the marginal distribution over return values of a stochastic function given inputs so that we can perform these computations.  Pyro accomplishes this by collecting a number of weighted execution traces of the function, then collapsing those traces into a histogram over possible return values given a particular set of arguments.\n",
    "\n",
    "Consider the following model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_normal_model(x):\n",
    "    z = pyro.sample(\"z\", dist.diagnormal, x, torch.ones(1))\n",
    "    y = pyro.sample(\"y\", dist.diagnormal, z, torch.ones(1))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting execution histories can be done either through sampling or, for models with only discrete latent variables, exact enumeration.  To create a basic importance sampler over execution traces, we can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posterior = pyro.infer.Importance(normal_normal_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`posterior` is not a particularly useful object on its own, though advanced users can call it with arguments for `normal_normal_model` to sample a raw execution trace.  Instead, `posterior` must be consumed by `pyro.infer.Marginal` to create a primitive stochastic function with the same input and output types as `normal_normal_model`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "marginal = pyro.infer.Marginal(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When called with an input `x`, `marginal` first uses `posterior` to generate a sequence of weighted execution traces given `x`, then builds a histogram over return values from the traces, and finally returns a sample drawn from the histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning models on data\n",
    "\n",
    "The real utility of probabilistic programming is in the ability to condition generative models on observed data. In Pyro, we separate the expression of conditioning from its evaluation via inference, making it possible to write a model once and condition it on many different observations.\n",
    "\n",
    "Consider `normal_normal_model` once again.  Suppose we want to sample from the marginal distribution of `z` given input `x = 0`, but now we have observed that `y == 1`.  Pyro provides the function `pyro.condition` to allow us to constrain the values of sample statements.  `pyro.condition` is a higher-order function that takes a model and a dictionary of data and returns a new model that has the same input and output signatures but always uses the same value at observed `sample` statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conditioned_normal_normal_model = pyro.condition(normal_normal_model, data={\"y\": torch.ones(1)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it behaves just like an ordinary Python function, conditioning can be deferred or parametrized with Python's `lambda`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deferred_conditioned_normal_normal_model = \\\n",
    "    lambda data, *args, **kwargs: pyro.condition(model, data=data)(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases it might be more convenient to pass observations directly to individual `pyro.sample` statements instead of using `pyro.condition`.  The optional `obs` kwarg is reserved by `pyro.sample` for that purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# equivalent to pyro.condition(model, data={\"y\": torch.ones(1)})\n",
    "def conditioned_normal_normal_model_2(x):\n",
    "    z = pyro.sample(\"z\", diagnormal, x, torch.ones(1))\n",
    "    y = pyro.sample(\"y\", diagnormal, z, torch.ones(1), obs=torch.ones(1)) # here we attach an observation y == 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, hardcoding is not usually recommended due to its invasive non-compositional nature.  By contrast, using `pyro.condition` and `pyro.do`, conditioning and intervention may be mixed and composed freely to form multiple complex queries on probabilistic models without modifying the underlying model.  The only restriction is that a single site may only be constrained once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model2():\n",
    "    mu = pyro.sample(\"mu\", diagnormal, torch.zeros(1), torch.ones(1))\n",
    "    sigma = torch.exp(pyro.sample(\"log_sigma\", diagnormal, torch.zeros(1), torch.ones(1)))\n",
    "    x = pyro.sample(\"x\", diagnormal, mu, sigma)\n",
    "    return sigma\n",
    "\n",
    "# conditioning composes: \n",
    "# the following are all equivalent and do not interfere with each other\n",
    "conditioned_model2_1 = pyro.condition(pyro.condition(model2, data={\"mu\": torch.ones(1)}), data={\"x\": torch.ones(1)})\n",
    "conditioned_model2_2 = pyro.condition(pyro.condition(model2, data={\"x\": torch.ones(1)}), data={\"mu\": torch.ones(1)})\n",
    "conditioned_model2_3 = pyro.condition(model2, data={\"x\": torch.ones(1), \"mu\": torch.ones(1)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flexible approximate inference with guide functions\n",
    "\n",
    "Consider `deferred_conditioned_normal_normal_model`.  Now that we have constrained `y` against some data, we can approximate the marginal distribution over `z` given `x` and `y == data` with importance sampling.  \n",
    "\n",
    "`pyro.infer.Importance` allows us to use arbitrary stochastic functions, which we will call *guide functions*, as proposal distributions, provided that:\n",
    "1. all unobserved sample statements that appear in the model appear in the guide.\n",
    "2. the guide has the same input signature as the model\n",
    "\n",
    "The guide function should be chosen so that it closely approximates the distribution over all unobserved `sample` statements in the model.  The simplest guide for `deferred_conditioned_normal_normal_model` matches the prior distribution over `z`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_normal_prior_guide(y, x):\n",
    "    return pyro.sample(\"z\", diagnormal, x, torch.ones(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the prior distribution is generally not a very good model of the posterior distribution.  For `normal_normal_model`, it can be shown that the posterior distribution over `z` given `x` can be written as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_normal_posterior_guide(y, x):\n",
    "    a = (x + torch.sum(y)) / (torch.size(y, 0) + 1.0)\n",
    "    b = torch.ones(1) / (torch.size(y, 0) + 1.0)\n",
    "    return pyro.sample(\"z\", diagnormal, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guide functions can serve as programmable proposal distributions for importance sampling, rejection sampling, sequential Monte Carlo, MCMC, and independent Metropolis-Hastings, and as variational distributions for stochastic variational inference.  However, currently only importance sampling and stochastic variational inference are implemented in Pyro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrized stochastic functions and variational inference\n",
    "\n",
    "Although we could write out the exact posterior distribution for `normal_normal_model`, in general it is intractable to specify a guide that is a good approximation to the posterior distribution of an arbitrary conditioned stochastic function.  What we can do instead is use the top-level function `pyro.param` to specify a *family* of guides indexed by named parameters, and search for the member of that family that is the best approximation.  This approach to approximate inference is called *variational inference*.\n",
    "\n",
    "`pyro.param` is a frontend for Pyro's key-value *parameter store*, described in more detail in the SVI tutorial (LINK).  It is in effect a memoized tensor constructor.  Like `pyro.sample`, it is always called with a name as its first argument.  The first time `pyro.param` is called with a particular name, it stores its argument in the parameter store and then returns that value.  After that, when it is called with that name, it returns the value from the parameter store regardless of any other arguments.\n",
    "\n",
    "For example, we can parametrize `a` and `b` in `normal_normal_posterior_guide` instead of specifying them by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_normal_parametrized_guide(y, x):\n",
    "    a = pyro.param(\"a\", torch.randn(1))\n",
    "    b = pyro.param(\"b\", torch.exp(torch.randn(1)))\n",
    "    return pyro.sample(\"z\", diagnormal, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyro is built to enable *stochastic variational inference*, where parameters are always real-valued tensors, we compute Monte Carlo estimates of a loss function from samples of execution histories of the model and guide, and use stochastic gradient descent to search for the optimal parameters.  Combining stochastic gradient descent with PyTorch's GPU-accelerated tensor math and automatic differentiation allows us to scale variational inference to very high-dimensional parameter spaces and massive datasets.  Pyro's SVI functionality is described in detail in the SVI tutorial (LINK)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "In the Variational Autoencoder tutorial (LINK), we'll augment `normal_normal_model` and `normal_normal_parametrized_guide` with deep neural networks and use stochastic variational inference to build a generative model of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
